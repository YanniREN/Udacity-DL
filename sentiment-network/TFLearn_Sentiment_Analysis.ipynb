{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with TFLearn\n",
    "\n",
    "In this notebook, we'll continue Andrew Trask's work by building a network for sentiment analysis on the movie review data. Instead of a network written with Numpy, we'll be using [TFLearn](http://tflearn.org/), a high-level library built on top of TensorFlow. TFLearn makes it simpler to build networks just by defining the layers. It takes care of most of the details for you.\n",
    "\n",
    "We'll start off by importing all the modules we'll need, then load and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Following along with Andrew, our goal here is to convert our reviews into word vectors. The word vectors will have elements representing words in the total vocabulary. If the second position represents the word 'the', for each review we'll count up the number of times 'the' appears in the text and set the second position to that count. I'll show you examples as we build the input data from the reviews data. Check out Andrew's notebook and video for more about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "Use the pandas library to read the reviews and postive/negative labels from comma-separated files. The data we're using has already been preprocessed a bit and we know it uses only lower case characters. If we were working from raw data, where we didn't know it was all lower case, we would want to add a step here to convert it. That's so we treat different variations of the same word, like `The`, `the`, and `THE`, all the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting word frequency\n",
    "\n",
    "To start off we'll need to count how often each word appears in the data. We'll use this count to create a vocabulary we'll use to encode the review data. This resulting count is known as a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model). We'll use it to select our vocabulary and build the word vectors. You should have seen how to do this in Andrew's lesson. Try to implement it here using the [Counter class](https://docs.python.org/2/library/collections.html#collections.Counter).\n",
    "\n",
    "> **Exercise:** Create the bag of words from the reviews data and assign it to `total_counts`. The reviews are stores in the `reviews` [Pandas DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html). If you want the reviews as a Numpy array, use `reviews.values`. You can iterate through the rows in the DataFrame with `for idx, row in reviews.iterrows():` ([documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.iterrows.html)). When you break up the reviews into words, use `.split(' ')` instead of `.split()` so your results match ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high is a cartoon comedy . it ran at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>homelessness  or houselessness as george carli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>airport    starts as a brand new luxury    pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brilliant over  acting by lesley ann warren . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>this film lacked something i couldn  t put my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>this is easily the most underrated film inn th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sorry everyone    i know this is supposed to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this is not the typical mel brooks film . it w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>this isn  t the comedic robin williams  nor is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>it appears that many critics find the idea o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>yes its an art . . . to successfully make a sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the second attempt by a new york intellectual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>in this  critically acclaimed psychological th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i don  t know who to blame  the timid writers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>the night listener           robin williams  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>this film is mediocre at best . angie harmon i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>you know  robin williams  god bless him  is co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>the film is bad . there is no other way to say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>when i first read armistead maupins story i wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>this film is one giant pant load . paul schrad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i liked the film . some of the action scenes w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>the plot for descent  if it actually can be ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>there are many illnesses born in the mind of m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>plot is not worth discussion even if it hints ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>i enjoyed the night listener very much . it  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>this film is about a male escort getting invol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>the night listener is probably not one of will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>this movie must be in line for the most boring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>i had a chance to see a screening of this movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>i had the misfortune to watch this rubbish on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>this is a really interesting movie . it is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>it  s pretty bad when the generic movie synops...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>i saw the movie recently and really liked it ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>having watched this movie on the scifi channel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>i thought this movie was hysterical . i have w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>first off  i  m not here to dog this movie . i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>. . . this is a classic with so many great di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>ah yez  the sci fi channel produces yeti anoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>the most hillarious and funny brooks movie i e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>yeti curse of the snow demon starts aboard a p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>life stinks  is a parody of life and death  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>hmmm  a sports team is in a plane crash  gets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>this is the kind of film you want to see with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>i saw this piece of garbage on amc last night ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>i have not read the other comments on the film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>although the production and jerry jameson  s d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>life stinks       was a step below mel brooks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>capt . gallagher  lemmon  and flight attendant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>seeing as the vote average was pretty low  and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>towards the end of the movie  i felt it was to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>the plot had some wretched  unbelievable twist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>this is the kind of movie that my enemies cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>i am amazed at how this movie  and most others...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>i saw  descent  last night at the stockholm fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>a christmas together actually came before my t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>some films that you pick up for a pound turn o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>working  class romantic drama from director ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>this is one of the dumbest films  i  ve ever s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0      bromwell high is a cartoon comedy . it ran at ...\n",
       "1      story of a man who has unnatural feelings for ...\n",
       "2      homelessness  or houselessness as george carli...\n",
       "3      airport    starts as a brand new luxury    pla...\n",
       "4      brilliant over  acting by lesley ann warren . ...\n",
       "5      this film lacked something i couldn  t put my ...\n",
       "6      this is easily the most underrated film inn th...\n",
       "7      sorry everyone    i know this is supposed to b...\n",
       "8      this is not the typical mel brooks film . it w...\n",
       "9      when i was little my parents took me along to ...\n",
       "10     this isn  t the comedic robin williams  nor is...\n",
       "11       it appears that many critics find the idea o...\n",
       "12     yes its an art . . . to successfully make a sl...\n",
       "13     the second attempt by a new york intellectual ...\n",
       "14     in this  critically acclaimed psychological th...\n",
       "15     i don  t know who to blame  the timid writers ...\n",
       "16     the night listener           robin williams  t...\n",
       "17     this film is mediocre at best . angie harmon i...\n",
       "18     you know  robin williams  god bless him  is co...\n",
       "19     the film is bad . there is no other way to say...\n",
       "20     when i first read armistead maupins story i wa...\n",
       "21     this film is one giant pant load . paul schrad...\n",
       "22     i liked the film . some of the action scenes w...\n",
       "23     the plot for descent  if it actually can be ca...\n",
       "24     there are many illnesses born in the mind of m...\n",
       "25     plot is not worth discussion even if it hints ...\n",
       "26     i enjoyed the night listener very much . it  s...\n",
       "27     this film is about a male escort getting invol...\n",
       "28     the night listener is probably not one of will...\n",
       "29     this movie must be in line for the most boring...\n",
       "...                                                  ...\n",
       "24970  i had a chance to see a screening of this movi...\n",
       "24971  i had the misfortune to watch this rubbish on ...\n",
       "24972  this is a really interesting movie . it is an ...\n",
       "24973  it  s pretty bad when the generic movie synops...\n",
       "24974  i saw the movie recently and really liked it ....\n",
       "24975  having watched this movie on the scifi channel...\n",
       "24976  i thought this movie was hysterical . i have w...\n",
       "24977  first off  i  m not here to dog this movie . i...\n",
       "24978   . . . this is a classic with so many great di...\n",
       "24979  ah yez  the sci fi channel produces yeti anoth...\n",
       "24980  the most hillarious and funny brooks movie i e...\n",
       "24981  yeti curse of the snow demon starts aboard a p...\n",
       "24982    life stinks  is a parody of life and death  ...\n",
       "24983  hmmm  a sports team is in a plane crash  gets ...\n",
       "24984  this is the kind of film you want to see with ...\n",
       "24985  i saw this piece of garbage on amc last night ...\n",
       "24986  i have not read the other comments on the film...\n",
       "24987  although the production and jerry jameson  s d...\n",
       "24988  life stinks       was a step below mel brooks ...\n",
       "24989  capt . gallagher  lemmon  and flight attendant...\n",
       "24990  seeing as the vote average was pretty low  and...\n",
       "24991  towards the end of the movie  i felt it was to...\n",
       "24992  the plot had some wretched  unbelievable twist...\n",
       "24993  this is the kind of movie that my enemies cont...\n",
       "24994  i am amazed at how this movie  and most others...\n",
       "24995  i saw  descent  last night at the stockholm fi...\n",
       "24996  a christmas together actually came before my t...\n",
       "24997  some films that you pick up for a pound turn o...\n",
       "24998  working  class romantic drama from director ma...\n",
       "24999  this is one of the dumbest films  i  ve ever s...\n",
       "\n",
       "[25000 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "type(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "\n",
    "reviews = reviews.T\n",
    "total_counts_set = set()# bag of words here\n",
    "for review in reviews[0]:\n",
    "    for word in review.split(\" \"):\n",
    "        total_counts_set.add(word)\n",
    "total_counts_set = list(total_counts_set)\n",
    "\"\"\"        \n",
    "total_counts = Counter()\n",
    "for i in range(len(total_counts_set)):\n",
    "    word = total_counts_set[i]\n",
    "    for j in range(len(total_counts_set)):\n",
    "        if (word == total_counts_set[j]):            \n",
    "            total_counts[word] += 1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Total words in data set: \", len(total_counts_set))\n",
    "#print(total_counts_set.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total_counts = Counter()\n",
    "for i in range(len(total_counts_set)):\n",
    "    word = total_counts_set[i]\n",
    "    for j in range(len(total_counts_set)):\n",
    "        if (word == total_counts_set[j]):            \n",
    "            total_counts[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#total_counts_set\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in data set:  74074\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#reviews = reviews.T\n",
    "total_counts = Counter()# bag of words here\n",
    "for _ , row in reviews.iterrows():\n",
    "    total_counts.update(row[0].split(' '))\n",
    "print(\"Total words in data set: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'': 1111930,\n",
       "         'ed': 326,\n",
       "         'assistants': 19,\n",
       "         'sharpens': 1,\n",
       "         'familial': 14,\n",
       "         'hoy': 1,\n",
       "         'saxophone': 11,\n",
       "         'iranian': 50,\n",
       "         'medalist': 1,\n",
       "         'arthropods': 1,\n",
       "         'joyfully': 5,\n",
       "         'derelicts': 2,\n",
       "         'derrek': 1,\n",
       "         'drawback': 27,\n",
       "         'quinnell': 2,\n",
       "         'surfers': 33,\n",
       "         'monosyllables': 2,\n",
       "         'seashell': 1,\n",
       "         'existentialist': 7,\n",
       "         'spawns': 6,\n",
       "         'oedipal': 10,\n",
       "         'deputized': 1,\n",
       "         'oyama': 2,\n",
       "         'extrication': 1,\n",
       "         'naivity': 1,\n",
       "         'cassella': 1,\n",
       "         'ravindra': 1,\n",
       "         'impactful': 8,\n",
       "         'countdowns': 1,\n",
       "         'vivienne': 2,\n",
       "         'melodies': 20,\n",
       "         'sistahood': 1,\n",
       "         'anaesthesia': 1,\n",
       "         'geraldine': 26,\n",
       "         'username': 2,\n",
       "         'deathrap': 1,\n",
       "         'whooping': 4,\n",
       "         'crescent': 1,\n",
       "         'literally': 468,\n",
       "         'autistic': 27,\n",
       "         'comprehension': 24,\n",
       "         'alana': 1,\n",
       "         'anita': 48,\n",
       "         'senseless': 101,\n",
       "         'tomiche': 4,\n",
       "         'dignity': 118,\n",
       "         'sainthood': 2,\n",
       "         'astaire': 132,\n",
       "         'philippine': 4,\n",
       "         'yester': 1,\n",
       "         'ensuring': 14,\n",
       "         'unhappy': 95,\n",
       "         'superficially': 16,\n",
       "         'featurettes': 10,\n",
       "         'magnets': 2,\n",
       "         'pff': 1,\n",
       "         'amps': 2,\n",
       "         'atlante': 1,\n",
       "         'sourced': 4,\n",
       "         'ordeal': 52,\n",
       "         'reheated': 1,\n",
       "         'broek': 1,\n",
       "         'cede': 1,\n",
       "         'babaganoosh': 1,\n",
       "         'apply': 55,\n",
       "         'gamer': 7,\n",
       "         'dem': 1,\n",
       "         'threefold': 1,\n",
       "         'rebelious': 1,\n",
       "         'ponies': 1,\n",
       "         'assign': 7,\n",
       "         'vanquish': 2,\n",
       "         'admits': 46,\n",
       "         'imposition': 4,\n",
       "         'restart': 6,\n",
       "         'foreigner': 19,\n",
       "         'degenerates': 22,\n",
       "         'annuls': 1,\n",
       "         'jhutsi': 1,\n",
       "         'dragonballz': 1,\n",
       "         'boisterous': 10,\n",
       "         'warp': 20,\n",
       "         'edtv': 1,\n",
       "         'mexicanos': 1,\n",
       "         'houses': 105,\n",
       "         'cocked': 4,\n",
       "         'mercies': 5,\n",
       "         'waterfront': 30,\n",
       "         'beckons': 2,\n",
       "         'hob': 1,\n",
       "         'dzundza': 3,\n",
       "         'shiver': 12,\n",
       "         'wafer': 15,\n",
       "         'sources': 54,\n",
       "         'cruellest': 2,\n",
       "         'disturbia': 1,\n",
       "         'playstation': 8,\n",
       "         'hyperbolic': 1,\n",
       "         'runmanian': 1,\n",
       "         'snowflake': 1,\n",
       "         'tindle': 1,\n",
       "         'empirical': 1,\n",
       "         'mung': 2,\n",
       "         'meso': 6,\n",
       "         'gargan': 2,\n",
       "         'strangly': 1,\n",
       "         'headey': 5,\n",
       "         'criminally': 33,\n",
       "         'tbi': 2,\n",
       "         'coreys': 1,\n",
       "         'arsenical': 1,\n",
       "         'misery': 92,\n",
       "         'offensives': 1,\n",
       "         'obfuscated': 1,\n",
       "         'moebius': 1,\n",
       "         'ladd': 13,\n",
       "         'usn': 1,\n",
       "         'anticlimactic': 16,\n",
       "         'robust': 20,\n",
       "         'anan': 1,\n",
       "         'uneven': 107,\n",
       "         'rrhs': 1,\n",
       "         'installments': 20,\n",
       "         'sledgehammers': 1,\n",
       "         'wilcoxon': 2,\n",
       "         'bhamra': 4,\n",
       "         'murray': 81,\n",
       "         'orang': 3,\n",
       "         'halton': 1,\n",
       "         'reprint': 1,\n",
       "         'getaways': 2,\n",
       "         'velizar': 1,\n",
       "         'huttner': 1,\n",
       "         'interestingly': 69,\n",
       "         'pitiful': 76,\n",
       "         'avante': 2,\n",
       "         'awkrawrd': 1,\n",
       "         'heather': 40,\n",
       "         'hurt': 382,\n",
       "         'sais': 5,\n",
       "         'mcanally': 15,\n",
       "         'humanistic': 13,\n",
       "         'space': 755,\n",
       "         'phenomenon': 69,\n",
       "         'transformative': 5,\n",
       "         'wellesian': 3,\n",
       "         'truckload': 6,\n",
       "         'abysmal': 98,\n",
       "         'berkeley': 58,\n",
       "         'necron': 1,\n",
       "         'kerry': 31,\n",
       "         'antonia': 7,\n",
       "         'reputation': 191,\n",
       "         'backstreets': 1,\n",
       "         'reicher': 1,\n",
       "         'midst': 49,\n",
       "         'shrouding': 1,\n",
       "         'ison': 1,\n",
       "         'prc': 13,\n",
       "         'hs': 1,\n",
       "         'spredakos': 1,\n",
       "         'spliff': 1,\n",
       "         'lowitz': 1,\n",
       "         'malplaced': 1,\n",
       "         'ramping': 1,\n",
       "         'acosta': 4,\n",
       "         'miraglittoa': 1,\n",
       "         'murvyn': 4,\n",
       "         'meanders': 24,\n",
       "         'pepperhaus': 1,\n",
       "         'edina': 1,\n",
       "         'hyperactive': 13,\n",
       "         'relearn': 1,\n",
       "         'rosalione': 1,\n",
       "         'hugs': 9,\n",
       "         'unicorn': 7,\n",
       "         'warholite': 1,\n",
       "         'pentagon': 12,\n",
       "         'malkovich': 9,\n",
       "         'masturbated': 1,\n",
       "         'modest': 62,\n",
       "         'thinkthey': 1,\n",
       "         'devane': 4,\n",
       "         'sixtyish': 2,\n",
       "         'emigres': 2,\n",
       "         'frits': 10,\n",
       "         'marginalisation': 1,\n",
       "         'italia': 2,\n",
       "         'frankenfish': 1,\n",
       "         'olosio': 1,\n",
       "         'monochrome': 14,\n",
       "         'mafioso': 9,\n",
       "         'ivanhoe': 2,\n",
       "         'bahamas': 1,\n",
       "         'perovitch': 1,\n",
       "         'bharti': 4,\n",
       "         'deuce': 7,\n",
       "         'subscriber': 1,\n",
       "         'sympathizers': 7,\n",
       "         'len': 4,\n",
       "         'selective': 13,\n",
       "         'briefly': 136,\n",
       "         'plethora': 22,\n",
       "         'bonner': 1,\n",
       "         'disregards': 9,\n",
       "         'insinuated': 2,\n",
       "         'wisetake': 1,\n",
       "         'kounen': 1,\n",
       "         'congenial': 2,\n",
       "         'settle': 104,\n",
       "         'trumph': 1,\n",
       "         'bille': 7,\n",
       "         'roc': 3,\n",
       "         'overgeneralizing': 1,\n",
       "         'laces': 6,\n",
       "         'prospering': 1,\n",
       "         'extreamly': 1,\n",
       "         'logics': 2,\n",
       "         'biplane': 3,\n",
       "         'narrows': 7,\n",
       "         'jump': 300,\n",
       "         'chocolate': 35,\n",
       "         'costy': 1,\n",
       "         'napper': 1,\n",
       "         'kidnappers': 19,\n",
       "         'shita': 1,\n",
       "         'gaffes': 5,\n",
       "         'tethers': 1,\n",
       "         'shanghainese': 1,\n",
       "         'worrying': 41,\n",
       "         'concurrently': 1,\n",
       "         'glorified': 27,\n",
       "         'blight': 6,\n",
       "         'dialectics': 1,\n",
       "         'shhhhh': 1,\n",
       "         'dignitary': 2,\n",
       "         'annapolis': 1,\n",
       "         'whitch': 2,\n",
       "         'goading': 1,\n",
       "         'horrortitles': 1,\n",
       "         'landowner': 8,\n",
       "         'romance': 694,\n",
       "         'tenet': 1,\n",
       "         'lexus': 1,\n",
       "         'warm': 227,\n",
       "         'frisky': 5,\n",
       "         'kistofferson': 1,\n",
       "         'juxtapositions': 3,\n",
       "         'inverts': 1,\n",
       "         'papel': 1,\n",
       "         'unconventional': 44,\n",
       "         'wimmer': 2,\n",
       "         'affecting': 44,\n",
       "         'peacefully': 6,\n",
       "         'mackey': 1,\n",
       "         'mopped': 1,\n",
       "         'mdm': 1,\n",
       "         'balsam': 15,\n",
       "         'bangla': 2,\n",
       "         'lecture': 34,\n",
       "         'sanitation': 4,\n",
       "         'bobs': 5,\n",
       "         'upendra': 2,\n",
       "         'cinematically': 14,\n",
       "         'zealanders': 4,\n",
       "         'fannn': 1,\n",
       "         'krocodylus': 2,\n",
       "         'kascier': 1,\n",
       "         'sturm': 3,\n",
       "         'lordly': 2,\n",
       "         'rainmakers': 2,\n",
       "         'frock': 5,\n",
       "         'woodfin': 1,\n",
       "         'entrant': 1,\n",
       "         'balme': 1,\n",
       "         'altercations': 1,\n",
       "         'bravely': 17,\n",
       "         'korolev': 5,\n",
       "         'zeon': 7,\n",
       "         'homeowner': 2,\n",
       "         'coos': 2,\n",
       "         'elaborate': 118,\n",
       "         'irrationally': 2,\n",
       "         'misty': 48,\n",
       "         'ancestry': 16,\n",
       "         'baio': 13,\n",
       "         'bark': 5,\n",
       "         'mavens': 12,\n",
       "         'patches': 11,\n",
       "         'stages': 71,\n",
       "         'ghostwriting': 1,\n",
       "         'instance': 289,\n",
       "         'coach': 102,\n",
       "         'kidman': 72,\n",
       "         'gol': 1,\n",
       "         'queue': 14,\n",
       "         'kimmy': 5,\n",
       "         'halfwit': 1,\n",
       "         'dood': 2,\n",
       "         'awesomeness': 8,\n",
       "         'afew': 1,\n",
       "         'goosebumps': 9,\n",
       "         'westing': 1,\n",
       "         'prefer': 175,\n",
       "         'moustache': 24,\n",
       "         'cherise': 1,\n",
       "         'geddes': 13,\n",
       "         'drac': 2,\n",
       "         'underwent': 5,\n",
       "         'centenary': 2,\n",
       "         'argonne': 4,\n",
       "         'lintz': 3,\n",
       "         'tastes': 78,\n",
       "         'terrorize': 18,\n",
       "         'marriage': 417,\n",
       "         'definetly': 9,\n",
       "         'excorism': 1,\n",
       "         'assult': 1,\n",
       "         'guffawed': 2,\n",
       "         'signalman': 4,\n",
       "         'zalman': 1,\n",
       "         'sweetish': 1,\n",
       "         'examining': 13,\n",
       "         'primo': 6,\n",
       "         'pyche': 1,\n",
       "         'recites': 9,\n",
       "         'stressed': 24,\n",
       "         'yule': 1,\n",
       "         'impulse': 28,\n",
       "         'brando': 159,\n",
       "         'inhumane': 9,\n",
       "         'uncover': 28,\n",
       "         'descriptions': 20,\n",
       "         'venus': 17,\n",
       "         'shenk': 2,\n",
       "         'decamp': 1,\n",
       "         'chiller': 28,\n",
       "         'learnfrom': 1,\n",
       "         'eachother': 4,\n",
       "         'wished': 95,\n",
       "         'telescoping': 3,\n",
       "         'commie': 17,\n",
       "         'floodgates': 1,\n",
       "         'maars': 1,\n",
       "         'ensnared': 1,\n",
       "         'hatter': 3,\n",
       "         'tryfon': 1,\n",
       "         'untwining': 1,\n",
       "         'rigby': 1,\n",
       "         'cliche': 97,\n",
       "         'masiela': 3,\n",
       "         'hitchens': 1,\n",
       "         'emotive': 15,\n",
       "         'turncoats': 1,\n",
       "         'cowles': 1,\n",
       "         'ahistorical': 1,\n",
       "         'detached': 43,\n",
       "         'albniz': 3,\n",
       "         'youssef': 17,\n",
       "         'medicine': 52,\n",
       "         'isbn': 2,\n",
       "         'soulfully': 1,\n",
       "         'ad': 146,\n",
       "         'anchors': 39,\n",
       "         'keung': 1,\n",
       "         'smarminess': 1,\n",
       "         'montgomery': 22,\n",
       "         'tah': 1,\n",
       "         'svankmajer': 1,\n",
       "         'sous': 2,\n",
       "         'amillenialist': 1,\n",
       "         'melisa': 1,\n",
       "         'kurush': 1,\n",
       "         'degeneres': 1,\n",
       "         'goddamit': 1,\n",
       "         'degen': 1,\n",
       "         'forevermore': 1,\n",
       "         'pastiche': 22,\n",
       "         'nirvana': 8,\n",
       "         'judo': 5,\n",
       "         'facilitates': 3,\n",
       "         'waimea': 1,\n",
       "         'engage': 93,\n",
       "         'chanel': 1,\n",
       "         'lederer': 1,\n",
       "         'lanie': 1,\n",
       "         'digitised': 2,\n",
       "         'winking': 7,\n",
       "         'listing': 33,\n",
       "         'troubling': 18,\n",
       "         'antiques': 9,\n",
       "         'nuyen': 1,\n",
       "         'respond': 41,\n",
       "         'threateningly': 2,\n",
       "         'shekar': 1,\n",
       "         'contest': 83,\n",
       "         'oooomph': 1,\n",
       "         'exclusives': 1,\n",
       "         'basis': 169,\n",
       "         'jonny': 23,\n",
       "         'dripped': 29,\n",
       "         'farnham': 2,\n",
       "         'dollari': 1,\n",
       "         'chosson': 1,\n",
       "         'grossness': 8,\n",
       "         'tatooed': 1,\n",
       "         'lambada': 1,\n",
       "         'engulfed': 13,\n",
       "         'rapids': 7,\n",
       "         'tankers': 1,\n",
       "         'sharron': 3,\n",
       "         'arabella': 6,\n",
       "         'informal': 4,\n",
       "         'apharan': 1,\n",
       "         'cuddlesome': 1,\n",
       "         'rosebud': 4,\n",
       "         'raymonde': 2,\n",
       "         'wahtever': 1,\n",
       "         'coaching': 13,\n",
       "         'dithers': 1,\n",
       "         'postmodernistic': 1,\n",
       "         'aneta': 10,\n",
       "         'glued': 59,\n",
       "         'maddy': 25,\n",
       "         'sturdy': 16,\n",
       "         'furs': 5,\n",
       "         'congradulations': 1,\n",
       "         'backwords': 1,\n",
       "         'counterpoint': 21,\n",
       "         'conquistador': 2,\n",
       "         'overreaching': 1,\n",
       "         'clubberin': 2,\n",
       "         'dorfmann': 1,\n",
       "         'candela': 1,\n",
       "         'alliance': 50,\n",
       "         'mauritius': 2,\n",
       "         'cabby': 2,\n",
       "         'philandering': 8,\n",
       "         'superfun': 1,\n",
       "         'bogdanovic': 1,\n",
       "         'eaters': 21,\n",
       "         'conscious': 79,\n",
       "         'krissakes': 1,\n",
       "         'simpatico': 1,\n",
       "         'grams': 16,\n",
       "         'recklessness': 7,\n",
       "         'joshi': 6,\n",
       "         'slav': 1,\n",
       "         'wish': 957,\n",
       "         'figurine': 3,\n",
       "         'walker': 117,\n",
       "         'cable': 280,\n",
       "         'arthritic': 4,\n",
       "         'cots': 1,\n",
       "         'cockneys': 2,\n",
       "         'inane': 95,\n",
       "         'untainted': 2,\n",
       "         'paintings': 69,\n",
       "         'archangel': 3,\n",
       "         'swell': 28,\n",
       "         'tellings': 4,\n",
       "         'bd': 7,\n",
       "         'dresch': 1,\n",
       "         'dustin': 58,\n",
       "         'approval': 31,\n",
       "         'viewership': 1,\n",
       "         'psychoanalytical': 4,\n",
       "         'phlip': 1,\n",
       "         'ghastly': 43,\n",
       "         'pamphleteering': 1,\n",
       "         'scratchy': 8,\n",
       "         'armoury': 3,\n",
       "         'geico': 11,\n",
       "         'whiff': 10,\n",
       "         'gratitude': 22,\n",
       "         'eradicated': 4,\n",
       "         'profited': 3,\n",
       "         'manages': 583,\n",
       "         'spooked': 8,\n",
       "         'spools': 2,\n",
       "         'mattered': 21,\n",
       "         'badminton': 1,\n",
       "         'longs': 21,\n",
       "         'verses': 10,\n",
       "         'wei': 20,\n",
       "         'hombres': 1,\n",
       "         'winterich': 1,\n",
       "         'monomaniacal': 1,\n",
       "         'outraged': 21,\n",
       "         'lovin': 9,\n",
       "         'emphatic': 8,\n",
       "         'exclamation': 14,\n",
       "         'perfomance': 1,\n",
       "         'traffic': 67,\n",
       "         'energize': 1,\n",
       "         'merosable': 1,\n",
       "         'doer': 2,\n",
       "         'alf': 2,\n",
       "         'merchandise': 16,\n",
       "         'shoppe': 1,\n",
       "         'tem': 1,\n",
       "         'stablemate': 1,\n",
       "         'ayer': 1,\n",
       "         'waggoner': 4,\n",
       "         'babcock': 2,\n",
       "         'higson': 2,\n",
       "         'mai': 10,\n",
       "         'william': 596,\n",
       "         'carping': 4,\n",
       "         'mincing': 5,\n",
       "         'rodolfo': 3,\n",
       "         'vilified': 6,\n",
       "         'omarosa': 1,\n",
       "         'bigga': 1,\n",
       "         'secunda': 1,\n",
       "         'lelouch': 9,\n",
       "         'cinephile': 4,\n",
       "         'exterminate': 3,\n",
       "         'unfailing': 3,\n",
       "         'twirling': 6,\n",
       "         'burdens': 4,\n",
       "         'bind': 8,\n",
       "         'noticeable': 58,\n",
       "         'uplifter': 1,\n",
       "         'lancie': 1,\n",
       "         'fragglerock': 1,\n",
       "         'sematarty': 1,\n",
       "         'talks': 220,\n",
       "         'rebelled': 3,\n",
       "         'verging': 9,\n",
       "         'remmy': 1,\n",
       "         'markey': 1,\n",
       "         'wipes': 23,\n",
       "         'partakes': 2,\n",
       "         'viola': 9,\n",
       "         'notld': 1,\n",
       "         'revist': 1,\n",
       "         'exuding': 3,\n",
       "         'czechoslovakia': 9,\n",
       "         'decry': 3,\n",
       "         'hulking': 13,\n",
       "         'vinod': 2,\n",
       "         'procedural': 8,\n",
       "         'campmates': 1,\n",
       "         'mckenzies': 1,\n",
       "         'costarring': 1,\n",
       "         'indefatigable': 3,\n",
       "         'tweety': 3,\n",
       "         'booooooooooooring': 1,\n",
       "         'runnin': 3,\n",
       "         'trill': 1,\n",
       "         'dachshund': 1,\n",
       "         'crouther': 1,\n",
       "         'fulcis': 1,\n",
       "         'ups': 266,\n",
       "         'cadfile': 1,\n",
       "         'brewer': 2,\n",
       "         'forgiveness': 45,\n",
       "         'juxtaposition': 15,\n",
       "         'espace': 2,\n",
       "         'kanji': 1,\n",
       "         'roi': 2,\n",
       "         'synagogue': 3,\n",
       "         'withdrawl': 1,\n",
       "         'lager': 4,\n",
       "         'souler': 3,\n",
       "         'parochial': 2,\n",
       "         'ardh': 6,\n",
       "         'shaye': 4,\n",
       "         'caveman': 19,\n",
       "         'skilled': 54,\n",
       "         'diversions': 7,\n",
       "         'heaving': 11,\n",
       "         'whittington': 2,\n",
       "         'gollum': 10,\n",
       "         'agnostic': 9,\n",
       "         'potboiler': 16,\n",
       "         'dobermann': 3,\n",
       "         'confetti': 1,\n",
       "         'shaw': 100,\n",
       "         'onde': 1,\n",
       "         'snacks': 8,\n",
       "         'bon': 38,\n",
       "         'flung': 13,\n",
       "         'monograms': 2,\n",
       "         'frequencies': 2,\n",
       "         'fender': 8,\n",
       "         'dispersement': 1,\n",
       "         'dogpatch': 1,\n",
       "         'fellas': 13,\n",
       "         'braveness': 1,\n",
       "         'retaliate': 5,\n",
       "         'kaante': 2,\n",
       "         'thingamajig': 1,\n",
       "         'chattered': 1,\n",
       "         'gruen': 1,\n",
       "         'deb': 4,\n",
       "         'lightflash': 1,\n",
       "         'shortlived': 1,\n",
       "         'squeeing': 1,\n",
       "         'dysfunction': 12,\n",
       "         'other': 9163,\n",
       "         'ronnies': 2,\n",
       "         'lowprice': 1,\n",
       "         'georgio': 5,\n",
       "         'lilja': 2,\n",
       "         'correctional': 9,\n",
       "         'reisman': 1,\n",
       "         'pore': 3,\n",
       "         'nudist': 5,\n",
       "         'uninitiated': 14,\n",
       "         'aspidistra': 1,\n",
       "         'johntopping': 1,\n",
       "         'delerium': 1,\n",
       "         'unimaginative': 60,\n",
       "         'realities': 63,\n",
       "         'bookcase': 3,\n",
       "         'screech': 4,\n",
       "         'acclaims': 2,\n",
       "         'eberhard': 1,\n",
       "         'scarsely': 1,\n",
       "         'pilate': 6,\n",
       "         'woman': 2795,\n",
       "         'subordinate': 5,\n",
       "         'doooor': 1,\n",
       "         'orphans': 12,\n",
       "         'liqueur': 1,\n",
       "         'rawanda': 3,\n",
       "         'leopards': 2,\n",
       "         'critiscism': 1,\n",
       "         'mechnical': 1,\n",
       "         'orwelll': 1,\n",
       "         'xia': 1,\n",
       "         'navigator': 3,\n",
       "         'uni': 9,\n",
       "         'disbelievers': 1,\n",
       "         'coinsidence': 1,\n",
       "         'explosion': 112,\n",
       "         'highland': 6,\n",
       "         'meffert': 1,\n",
       "         'tropi': 3,\n",
       "         'whitmore': 4,\n",
       "         'dinged': 1,\n",
       "         'memorized': 16,\n",
       "         'jrg': 4,\n",
       "         'shearmur': 1,\n",
       "         'hungering': 1,\n",
       "         'tumblers': 1,\n",
       "         'balanchine': 10,\n",
       "         'centred': 19,\n",
       "         'meaningless': 108,\n",
       "         'courtrooms': 4,\n",
       "         'reappears': 5,\n",
       "         'zombie': 740,\n",
       "         'familiars': 1,\n",
       "         'strength': 245,\n",
       "         'ridiculous': 965,\n",
       "         'deserved': 291,\n",
       "         'inlay': 1,\n",
       "         'urf': 3,\n",
       "         'synecdoche': 2,\n",
       "         'destroyed': 178,\n",
       "         'feitshans': 1,\n",
       "         'hadith': 4,\n",
       "         'kiley': 15,\n",
       "         'lucifer': 22,\n",
       "         'caligula': 8,\n",
       "         'sneezing': 4,\n",
       "         'caressing': 10,\n",
       "         'misjudging': 1,\n",
       "         'showers': 13,\n",
       "         'ron': 183,\n",
       "         'howz': 1,\n",
       "         'sayori': 5,\n",
       "         'deliverer': 1,\n",
       "         'compilations': 4,\n",
       "         'delete': 4,\n",
       "         'pharmacy': 4,\n",
       "         'gonzo': 13,\n",
       "         'testa': 5,\n",
       "         'apiece': 3,\n",
       "         'cubensis': 1,\n",
       "         'audibly': 5,\n",
       "         'whopper': 2,\n",
       "         'fatefully': 1,\n",
       "         'van': 495,\n",
       "         'erased': 16,\n",
       "         'orlandi': 1,\n",
       "         'cabell': 8,\n",
       "         'chapter': 88,\n",
       "         'progresses': 91,\n",
       "         'channing': 14,\n",
       "         'blai': 2,\n",
       "         'mustapha': 1,\n",
       "         'cavernous': 3,\n",
       "         'sensibility': 49,\n",
       "         'rover': 2,\n",
       "         'pleasantvillesque': 1,\n",
       "         'mustard': 9,\n",
       "         'vctor': 1,\n",
       "         'blammo': 2,\n",
       "         'aubry': 1,\n",
       "         'drohkaal': 1,\n",
       "         'admission': 44,\n",
       "         'which': 12047,\n",
       "         'guff': 5,\n",
       "         'salt': 67,\n",
       "         'incoherently': 3,\n",
       "         'iceman': 1,\n",
       "         'belami': 1,\n",
       "         'liye': 1,\n",
       "         'saws': 4,\n",
       "         'bouncing': 29,\n",
       "         'straightened': 7,\n",
       "         'pilar': 3,\n",
       "         'calculating': 18,\n",
       "         'faaar': 1,\n",
       "         'propagandized': 2,\n",
       "         'shiniest': 1,\n",
       "         'roulette': 10,\n",
       "         'annelle': 1,\n",
       "         'montejo': 1,\n",
       "         'incapacity': 2,\n",
       "         'calzone': 1,\n",
       "         'mountaineering': 1,\n",
       "         'rationale': 10,\n",
       "         'darwininan': 1,\n",
       "         'coer': 1,\n",
       "         'arkansas': 7,\n",
       "         'uwe': 102,\n",
       "         'reawakened': 2,\n",
       "         'walshs': 1,\n",
       "         'shifty': 15,\n",
       "         'powermaster': 1,\n",
       "         'lorenzo': 28,\n",
       "         'steirs': 1,\n",
       "         'veins': 18,\n",
       "         'underpinnings': 12,\n",
       "         'youll': 4,\n",
       "         'approxiately': 1,\n",
       "         'mouthburster': 1,\n",
       "         'stir': 34,\n",
       "         'dang': 11,\n",
       "         'guardian': 50,\n",
       "         'births': 3,\n",
       "         'doncha': 1,\n",
       "         'cintematography': 1,\n",
       "         'hiccups': 5,\n",
       "         'wedge': 7,\n",
       "         'generis': 1,\n",
       "         'evangelist': 12,\n",
       "         'ostentatiously': 1,\n",
       "         'opportunity': 393,\n",
       "         'pueblo': 1,\n",
       "         'reaking': 1,\n",
       "         'educate': 21,\n",
       "         'corker': 3,\n",
       "         'open': 665,\n",
       "         'mouthing': 8,\n",
       "         'nicest': 6,\n",
       "         'centurians': 1,\n",
       "         'setna': 1,\n",
       "         'mellifluous': 1,\n",
       "         'singing': 520,\n",
       "         'rasps': 1,\n",
       "         'sawblade': 1,\n",
       "         'homesteading': 3,\n",
       "         'absoutley': 1,\n",
       "         'exploit': 58,\n",
       "         'vancruysen': 1,\n",
       "         'hokey': 70,\n",
       "         'solett': 1,\n",
       "         'maillot': 1,\n",
       "         'surprised': 802,\n",
       "         'memorizing': 3,\n",
       "         'citizenship': 1,\n",
       "         'free': 697,\n",
       "         'pcp': 2,\n",
       "         'dooooosie': 1,\n",
       "         'findings': 12,\n",
       "         'lowell': 15,\n",
       "         'principaly': 1,\n",
       "         'mentioning': 85,\n",
       "         'prologues': 11,\n",
       "         'humanitas': 1,\n",
       "         'headaches': 11,\n",
       "         'nagasaki': 2,\n",
       "         'luxury': 39,\n",
       "         'ruthlessly': 16,\n",
       "         'bondy': 1,\n",
       "         'unsatisfactory': 14,\n",
       "         'debussy': 2,\n",
       "         'residuals': 1,\n",
       "         'vocalised': 1,\n",
       "         'shying': 1,\n",
       "         'otherworldliness': 3,\n",
       "         'preyer': 1,\n",
       "         'unhittable': 1,\n",
       "         'nous': 2,\n",
       "         'fixer': 4,\n",
       "         'gallner': 1,\n",
       "         'informercial': 1,\n",
       "         'craptitude': 1,\n",
       "         'henchmen': 44,\n",
       "         'rejected': 73,\n",
       "         'raiding': 6,\n",
       "         'sweetie': 3,\n",
       "         'dattilo': 1,\n",
       "         'centralized': 3,\n",
       "         'despaaaaaair': 1,\n",
       "         'decayed': 5,\n",
       "         'pannings': 1,\n",
       "         'screwdriver': 5,\n",
       "         'extinguishers': 5,\n",
       "         'unrelentingly': 7,\n",
       "         'incestual': 1,\n",
       "         'alzheimers': 1,\n",
       "         'cyclical': 4,\n",
       "         'voudon': 1,\n",
       "         'doa': 8,\n",
       "         'whisked': 11,\n",
       "         'stinkeroo': 2,\n",
       "         'includesraymond': 1,\n",
       "         'appendage': 2,\n",
       "         'deadpool': 4,\n",
       "         'contro': 2,\n",
       "         'scraps': 8,\n",
       "         'pay': 610,\n",
       "         'enchelada': 1,\n",
       "         'falsifications': 1,\n",
       "         'filmy': 13,\n",
       "         'cunningly': 10,\n",
       "         'asserted': 5,\n",
       "         'thirlby': 2,\n",
       "         'greenman': 3,\n",
       "         'ruge': 6,\n",
       "         'project': 491,\n",
       "         'qi': 11,\n",
       "         'stack': 79,\n",
       "         'sweatin': 1,\n",
       "         'fredrich': 1,\n",
       "         'gumption': 3,\n",
       "         'battlements': 1,\n",
       "         'hyenas': 10,\n",
       "         'berenger': 39,\n",
       "         'forthright': 5,\n",
       "         'tuneless': 3,\n",
       "         'pipedream': 1,\n",
       "         'jogger': 4,\n",
       "         'incoming': 7,\n",
       "         'globalizing': 1,\n",
       "         'concentrated': 35,\n",
       "         'hilarious': 972,\n",
       "         'repulsed': 13,\n",
       "         'promisingly': 13,\n",
       "         'joycelyn': 6,\n",
       "         'seychelles': 1,\n",
       "         'publishers': 6,\n",
       "         'tricked': 39,\n",
       "         'practiced': 14,\n",
       "         'scctm': 1,\n",
       "         'thingy': 9,\n",
       "         'gloved': 9,\n",
       "         'vehicles': 61,\n",
       "         'physit': 1,\n",
       "         'expiration': 3,\n",
       "         'intensional': 1,\n",
       "         'detect': 12,\n",
       "         'projection': 31,\n",
       "         'fashioned': 138,\n",
       "         'gershon': 14,\n",
       "         'syvlie': 1,\n",
       "         'mukshin': 1,\n",
       "         'choices': 171,\n",
       "         'rivero': 3,\n",
       "         'damon': 88,\n",
       "         'gondek': 1,\n",
       "         'batchler': 1,\n",
       "         'visibility': 4,\n",
       "         'glynn': 3,\n",
       "         'heightened': 25,\n",
       "         'nicktoons': 2,\n",
       "         'calculation': 1,\n",
       "         'edythe': 1,\n",
       "         'sophistication': 39,\n",
       "         'stranglers': 2,\n",
       "         'chatter': 15,\n",
       "         'aztec': 36,\n",
       "         'blalock': 7,\n",
       "         'pyle': 14,\n",
       "         'improper': 6,\n",
       "         'discouraged': 9,\n",
       "         'fugitive': 34,\n",
       "         'unseemly': 5,\n",
       "         'nordic': 8,\n",
       "         'gapes': 1,\n",
       "         'gapers': 1,\n",
       "         'popularized': 4,\n",
       "         'tawnee': 3,\n",
       "         'feint': 1,\n",
       "         'greying': 1,\n",
       "         'consuela': 1,\n",
       "         'scuffed': 1,\n",
       "         'gulfax': 6,\n",
       "         'traudl': 5,\n",
       "         'staryou': 1,\n",
       "         'batter': 4,\n",
       "         'smart': 406,\n",
       "         'rosza': 4,\n",
       "         'fcker': 2,\n",
       "         'inflicting': 13,\n",
       "         'ambiguousthe': 1,\n",
       "         'untreated': 1,\n",
       "         'ghettoisation': 1,\n",
       "         'manghattan': 1,\n",
       "         'silverado': 3,\n",
       "         'leather': 66,\n",
       "         'generations': 80,\n",
       "         'pecan': 1,\n",
       "         'clobbered': 1,\n",
       "         'whittle': 2,\n",
       "         'elia': 20,\n",
       "         'sics': 1,\n",
       "         'polley': 1,\n",
       "         'astrological': 3,\n",
       "         'swashbuckling': 16,\n",
       "         'horor': 2,\n",
       "         'finley': 2,\n",
       "         'dominant': 25,\n",
       "         'endearingly': 13,\n",
       "         'kewl': 3,\n",
       "         'amnesiac': 5,\n",
       "         'judgement': 40,\n",
       "         'telly': 34,\n",
       "         'gizmo': 5,\n",
       "         'goerge': 1,\n",
       "         'anew': 7,\n",
       "         'sozzled': 1,\n",
       "         'mavis': 1,\n",
       "         'raspy': 10,\n",
       "         'incorrectness': 6,\n",
       "         'roofer': 1,\n",
       "         'lulls': 13,\n",
       "         'resourcecenter': 1,\n",
       "         'greedy': 91,\n",
       "         'fountainhead': 1,\n",
       "         'traumatised': 6,\n",
       "         'loader': 3,\n",
       "         'hungers': 1,\n",
       "         'simplebut': 1,\n",
       "         'videoteque': 1,\n",
       "         'caps': 12,\n",
       "         'prettiest': 5,\n",
       "         'conflicted': 38,\n",
       "         'poodles': 2,\n",
       "         'espouses': 4,\n",
       "         'contrasted': 24,\n",
       "         'mortis': 3,\n",
       "         'entrancingly': 1,\n",
       "         'complainer': 1,\n",
       "         'jingoistic': 7,\n",
       "         'clearest': 2,\n",
       "         'therefore': 335,\n",
       "         'mooching': 1,\n",
       "         'sellon': 1,\n",
       "         'surmising': 1,\n",
       "         'uneventfully': 2,\n",
       "         'visby': 1,\n",
       "         'brainlessly': 1,\n",
       "         'grittier': 7,\n",
       "         'flew': 38,\n",
       "         'stellwaggen': 1,\n",
       "         'will': 9211,\n",
       "         'peer': 16,\n",
       "         'campaigns': 8,\n",
       "         'grooms': 4,\n",
       "         'hoaxy': 1,\n",
       "         'jude': 48,\n",
       "         'toppling': 1,\n",
       "         'untranslated': 1,\n",
       "         'translations': 6,\n",
       "         'sania': 1,\n",
       "         'unfruitful': 2,\n",
       "         'nip': 7,\n",
       "         'broklynese': 3,\n",
       "         'monica': 44,\n",
       "         'blooms': 6,\n",
       "         'stories': 1180,\n",
       "         'scanning': 9,\n",
       "         'dalmatians': 23,\n",
       "         'lucked': 3,\n",
       "         'surrealistic': 21,\n",
       "         'hampering': 2,\n",
       "         'ina': 2,\n",
       "         'spacecrafts': 1,\n",
       "         'sprucing': 1,\n",
       "         'delmer': 3,\n",
       "         'cheerful': 45,\n",
       "         'pt': 6,\n",
       "         'zomedy': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the first 10000 most frequent words. As Andrew noted, most of the words in the vocabulary are rarely used so they will have little effect on our predictions. Below, we'll sort `vocab` by the count value and keep the 10000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'the', '.', 'and', 'a', 'of', 'to', 'is', 'br', 'it', 'in', 'i', 'this', 'that', 's', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'you', 'on', 't', 'not', 'he', 'are', 'his', 'have', 'be', 'one', 'all', 'at', 'they', 'by', 'an', 'who', 'so', 'from', 'like', 'there', 'her', 'or', 'just', 'about', 'out', 'if', 'has', 'what', 'some', 'good', 'can', 'more', 'she', 'when', 'very', 'up', 'time', 'no']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)[:10000]\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the last word in our vocabulary? We can use this to judge if 10000 is too few. If the last word is pretty common, we probably need to keep more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathsheba :  30\n"
     ]
    }
   ],
   "source": [
    "print(vocab[-1], ': ', total_counts[vocab[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last word in our vocabulary shows up in 30 reviews out of 25000. I think it's fair to say this is a tiny proportion of reviews. We are probably fine with this number of words.\n",
    "\n",
    "**Note:** When you run, you may see a different word from the one shown above, but it will also have the value `30`. That's because there are many words tied for that number of counts, and the `Counter` class does not guarantee which one will be returned in the case of a tie.\n",
    "\n",
    "Now for each review in the data, we'll make a word vector. First we need to make a mapping of word to index, pretty easy to do with a dictionary comprehension.\n",
    "\n",
    "> **Exercise:** Create a dictionary called `word2idx` that maps each word in the vocabulary to an index. The first word in `vocab` has index `0`, the second word has index `1`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'majority': 2142,\n",
       " 'loyal': 4268,\n",
       " 'messy': 5835,\n",
       " 'ed': 1632,\n",
       " 'ww': 4835,\n",
       " 'capacity': 7164,\n",
       " 'itchy': 9408,\n",
       " 'employees': 8056,\n",
       " 'hate': 771,\n",
       " 'contrary': 3773,\n",
       " 'exploration': 4567,\n",
       " 'faded': 7783,\n",
       " 'stereotypes': 2090,\n",
       " 'poses': 6703,\n",
       " 'slapped': 6781,\n",
       " 'zero': 1428,\n",
       " 'iranian': 7074,\n",
       " 'wits': 9296,\n",
       " 'hilariously': 4811,\n",
       " 'redeem': 5657,\n",
       " 'tape': 2179,\n",
       " 'insane': 2123,\n",
       " 'underneath': 5214,\n",
       " 'soylent': 6136,\n",
       " 'munchies': 9429,\n",
       " 'russ': 7388,\n",
       " 'andrew': 3142,\n",
       " 'controversy': 7052,\n",
       " 'warner': 2719,\n",
       " 'sexual': 845,\n",
       " 'krueger': 7612,\n",
       " 'surfers': 9321,\n",
       " 'unfair': 5082,\n",
       " 'devastating': 6357,\n",
       " 'until': 364,\n",
       " 'ambition': 5843,\n",
       " 'talented': 1000,\n",
       " 'clips': 2916,\n",
       " 'manhattan': 3661,\n",
       " 'stephanie': 6519,\n",
       " 'tame': 3912,\n",
       " 'discernible': 9218,\n",
       " 'tactics': 6855,\n",
       " 'responsibility': 4718,\n",
       " 'candidates': 9231,\n",
       " 'mentioned': 1028,\n",
       " 'december': 8160,\n",
       " 'pasolini': 8926,\n",
       " 'demonstrated': 6658,\n",
       " 'bones': 6183,\n",
       " 'kathryn': 5703,\n",
       " 'nephew': 4898,\n",
       " 'weapons': 2837,\n",
       " 'ago': 589,\n",
       " 'christine': 5430,\n",
       " 'shorter': 5807,\n",
       " 'capt': 9484,\n",
       " 'suspected': 6539,\n",
       " 'fraud': 7786,\n",
       " 'kentucky': 9409,\n",
       " 'legend': 1758,\n",
       " 'chorus': 4283,\n",
       " 'raunchy': 7701,\n",
       " 'zoom': 8916,\n",
       " 'wb': 9864,\n",
       " 'mitch': 5891,\n",
       " 'targeted': 7360,\n",
       " 'identities': 8514,\n",
       " 'painting': 3337,\n",
       " 'garland': 6201,\n",
       " 'middle': 642,\n",
       " 'sufficient': 7368,\n",
       " 'literally': 1200,\n",
       " 'overboard': 6659,\n",
       " 'murky': 7136,\n",
       " 'patch': 9784,\n",
       " 'banana': 9844,\n",
       " 'amongst': 2887,\n",
       " 'anita': 7268,\n",
       " 'horses': 3172,\n",
       " 'destroying': 4474,\n",
       " 'works': 487,\n",
       " 'senseless': 4220,\n",
       " 'exposes': 9697,\n",
       " 'dignity': 3710,\n",
       " 'modesty': 3629,\n",
       " 'egg': 8003,\n",
       " 'rugged': 9081,\n",
       " 'wrestling': 4147,\n",
       " 'astaire': 3390,\n",
       " 'somethings': 7481,\n",
       " 'beetle': 8113,\n",
       " 'dumber': 6648,\n",
       " 'version': 311,\n",
       " 'clunky': 7435,\n",
       " 'monologue': 6745,\n",
       " 'timberlake': 5694,\n",
       " 'electricity': 8927,\n",
       " 'approaching': 6297,\n",
       " 'crosby': 6184,\n",
       " 'floating': 4658,\n",
       " 'passion': 1779,\n",
       " 'bombed': 8362,\n",
       " 'twenty': 1764,\n",
       " 'mindless': 3035,\n",
       " 'irresistible': 8872,\n",
       " 'weight': 3318,\n",
       " 'total': 937,\n",
       " 'prefer': 2744,\n",
       " 'play': 293,\n",
       " 'marlon': 5700,\n",
       " 'languages': 8725,\n",
       " 'festivals': 7121,\n",
       " 'obsession': 2918,\n",
       " 'ordeal': 6894,\n",
       " 'write': 883,\n",
       " 'rental': 2337,\n",
       " 'influences': 8124,\n",
       " 'ears': 4306,\n",
       " 'relentlessly': 6767,\n",
       " 'understandable': 4364,\n",
       " 'savage': 3454,\n",
       " 'concert': 3198,\n",
       " 'petty': 4939,\n",
       " 'transforms': 8087,\n",
       " 'deserves': 994,\n",
       " 'routine': 2454,\n",
       " 'delicious': 6224,\n",
       " 'dance': 816,\n",
       " 'parties': 4761,\n",
       " 'fighter': 3913,\n",
       " 'defending': 7974,\n",
       " 'participate': 7650,\n",
       " 'parodies': 8058,\n",
       " 'trap': 3570,\n",
       " 'arrested': 3522,\n",
       " 'logo': 9131,\n",
       " 'twice': 1418,\n",
       " 'incredible': 1030,\n",
       " 'identify': 3432,\n",
       " 'exploit': 6368,\n",
       " 'eva': 3251,\n",
       " 'sci': 898,\n",
       " 'redneck': 7110,\n",
       " 'villainous': 7531,\n",
       " 'cult': 1173,\n",
       " 'personalities': 3002,\n",
       " 'roots': 4976,\n",
       " 'starred': 2651,\n",
       " 'crashes': 6104,\n",
       " 'specially': 4864,\n",
       " 'ally': 6172,\n",
       " 'error': 6021,\n",
       " 'facing': 4393,\n",
       " 'shirt': 3717,\n",
       " 'crashed': 7947,\n",
       " 'houses': 4079,\n",
       " 'direction': 452,\n",
       " 'scifi': 6137,\n",
       " 'company': 1103,\n",
       " 'exudes': 9537,\n",
       " 'inclusion': 6993,\n",
       " 'helen': 3068,\n",
       " 'length': 1601,\n",
       " 'lie': 2856,\n",
       " 'won': 379,\n",
       " 'explores': 5565,\n",
       " 'allows': 2044,\n",
       " 'became': 860,\n",
       " 'genetic': 8631,\n",
       " 'purpose': 1263,\n",
       " 'felix': 3665,\n",
       " 'harris': 2019,\n",
       " 'want': 182,\n",
       " 'vhs': 1841,\n",
       " 'reaching': 4440,\n",
       " 'projection': 9899,\n",
       " 'cliched': 6871,\n",
       " 'years': 155,\n",
       " 'morally': 5578,\n",
       " 'danger': 2331,\n",
       " 'access': 4557,\n",
       " 'core': 1995,\n",
       " 'sheeta': 9956,\n",
       " 'celebrated': 6454,\n",
       " 'richard': 723,\n",
       " 'sirk': 4514,\n",
       " 'holm': 9876,\n",
       " 'seem': 307,\n",
       " 'cares': 2221,\n",
       " 'lap': 9432,\n",
       " 'examples': 2684,\n",
       " 'terrible': 389,\n",
       " 'laugh': 457,\n",
       " 'cursed': 8167,\n",
       " 'offs': 5630,\n",
       " 'system': 1469,\n",
       " 'generation': 2168,\n",
       " 'musician': 5198,\n",
       " 'frat': 8090,\n",
       " 'commanding': 7500,\n",
       " 'zombies': 1099,\n",
       " 'illusion': 8698,\n",
       " 'putting': 1470,\n",
       " 'logical': 3608,\n",
       " 'consists': 3185,\n",
       " 'formula': 2039,\n",
       " 'lloyd': 3285,\n",
       " 'rapist': 5670,\n",
       " 'iii': 3353,\n",
       " 'unbelievably': 3781,\n",
       " 'educational': 4958,\n",
       " 'btw': 5868,\n",
       " 'transition': 4570,\n",
       " 'liar': 8594,\n",
       " 'student': 1407,\n",
       " 'imagery': 2633,\n",
       " 'criminally': 9322,\n",
       " 'booker': 8720,\n",
       " 'fellow': 1464,\n",
       " 'slack': 9750,\n",
       " 'storyline': 754,\n",
       " 'teacher': 1691,\n",
       " 'telling': 966,\n",
       " 'yelling': 4558,\n",
       " 'growth': 6066,\n",
       " 'laced': 8488,\n",
       " 'kindness': 9200,\n",
       " 'misery': 4540,\n",
       " 'north': 2338,\n",
       " 'interpret': 9455,\n",
       " 'hogan': 8791,\n",
       " 'statements': 6911,\n",
       " 'frozen': 7131,\n",
       " 'bomb': 2153,\n",
       " 'madly': 9994,\n",
       " 'franklin': 6402,\n",
       " 'outfit': 4248,\n",
       " 'monk': 4836,\n",
       " 'manipulation': 7486,\n",
       " 'chevy': 8383,\n",
       " 'uncredited': 8705,\n",
       " 'sweat': 7097,\n",
       " 'chuck': 3210,\n",
       " 'punch': 2783,\n",
       " 'uneven': 4016,\n",
       " 'writers': 891,\n",
       " 'economy': 8125,\n",
       " 'bonnie': 6838,\n",
       " 'forwarding': 9990,\n",
       " 'teddy': 8825,\n",
       " 'knight': 5262,\n",
       " 'batwoman': 8969,\n",
       " 'hiding': 3190,\n",
       " 'blandings': 7319,\n",
       " 'murray': 5021,\n",
       " 'together': 292,\n",
       " 'natalie': 5875,\n",
       " 'clutter': 8201,\n",
       " 'tyler': 5362,\n",
       " 'creating': 1837,\n",
       " 'vivah': 9188,\n",
       " 'cancer': 5320,\n",
       " 'talked': 3507,\n",
       " 'beer': 3601,\n",
       " 'interestingly': 5617,\n",
       " 'pitiful': 5246,\n",
       " 'creation': 3452,\n",
       " 'glenda': 8293,\n",
       " 'brazil': 3789,\n",
       " 'montana': 4423,\n",
       " 'sections': 7996,\n",
       " 'failing': 3689,\n",
       " 'pirate': 6277,\n",
       " 'macho': 5553,\n",
       " 'serve': 2860,\n",
       " 'sea': 1959,\n",
       " 'dawn': 3329,\n",
       " 'heather': 8179,\n",
       " 'forbidden': 3670,\n",
       " 'tries': 490,\n",
       " 'reese': 8126,\n",
       " 'hurt': 1432,\n",
       " 'eventual': 6298,\n",
       " 'laying': 7613,\n",
       " 'interview': 2685,\n",
       " 'attended': 6964,\n",
       " 'advantage': 3046,\n",
       " 'gesture': 9675,\n",
       " 'technically': 2503,\n",
       " 'could': 98,\n",
       " 'gets': 212,\n",
       " 'swim': 4947,\n",
       " 'preview': 4371,\n",
       " 'placed': 2593,\n",
       " 'concerned': 1930,\n",
       " 'tip': 5549,\n",
       " 'aura': 8632,\n",
       " 'brazilian': 7966,\n",
       " 'applause': 8609,\n",
       " 'mid': 1669,\n",
       " 'captive': 8337,\n",
       " 'edge': 1255,\n",
       " 'train': 1338,\n",
       " 'within': 729,\n",
       " 'spaghetti': 6482,\n",
       " 'gained': 6163,\n",
       " 'mixed': 1824,\n",
       " 'ford': 1734,\n",
       " 'mixing': 6037,\n",
       " 'pregnant': 2726,\n",
       " 'associate': 8075,\n",
       " 'expresses': 8568,\n",
       " 'extensive': 7361,\n",
       " 'simba': 8569,\n",
       " 'ecstasy': 8902,\n",
       " 'similarity': 7547,\n",
       " 'convinced': 2316,\n",
       " 'yard': 5017,\n",
       " 'broke': 3071,\n",
       " 'leia': 9256,\n",
       " 'barton': 8463,\n",
       " 'austin': 4998,\n",
       " 'greek': 3845,\n",
       " 'crawl': 8968,\n",
       " 'inmates': 6588,\n",
       " 'hints': 4148,\n",
       " 'rewarded': 7181,\n",
       " 'agent': 1502,\n",
       " 'gas': 2508,\n",
       " 'authors': 8549,\n",
       " 'featuring': 1872,\n",
       " 'trivia': 5554,\n",
       " 'hard': 251,\n",
       " 'test': 2163,\n",
       " 'sharon': 5149,\n",
       " 'joke': 947,\n",
       " 'fi': 896,\n",
       " 'kirsten': 7636,\n",
       " 'inferior': 4485,\n",
       " 'wears': 2870,\n",
       " 'edison': 7058,\n",
       " 'eats': 5712,\n",
       " 'goes': 267,\n",
       " 'satisfactory': 8746,\n",
       " 'eastwood': 3300,\n",
       " 'somber': 9722,\n",
       " 'elder': 8670,\n",
       " 'german': 1102,\n",
       " 'atrocious': 2496,\n",
       " 'miscast': 3200,\n",
       " 'combs': 7082,\n",
       " 'gather': 5159,\n",
       " 'programming': 8110,\n",
       " 'buster': 5028,\n",
       " 'holly': 4046,\n",
       " 'unsatisfying': 6569,\n",
       " 'witnesses': 4686,\n",
       " 'politicians': 7260,\n",
       " 'hungarian': 8906,\n",
       " 'preacher': 7733,\n",
       " 'arrogance': 7558,\n",
       " 'collaboration': 8351,\n",
       " 'pierre': 6465,\n",
       " 'yep': 6236,\n",
       " 'hoot': 6794,\n",
       " 'krishna': 9548,\n",
       " 'refreshing': 2406,\n",
       " 'sat': 1791,\n",
       " 'addict': 5993,\n",
       " 'dawson': 3433,\n",
       " 'mentions': 4913,\n",
       " 'scream': 1932,\n",
       " 'wow': 1286,\n",
       " 'million': 1390,\n",
       " 'once': 282,\n",
       " 'cue': 5335,\n",
       " 'who': 37,\n",
       " 'israeli': 6865,\n",
       " 'archive': 9301,\n",
       " 'dangerously': 7658,\n",
       " 'aiello': 9293,\n",
       " 'bum': 9514,\n",
       " 'sometimes': 509,\n",
       " 'faithful': 2716,\n",
       " 'ghost': 1171,\n",
       " 'denmark': 9977,\n",
       " 'boobs': 8084,\n",
       " 'peaceful': 6567,\n",
       " 'attendant': 8292,\n",
       " 'weapon': 3122,\n",
       " 'spinning': 8755,\n",
       " 'chicken': 5067,\n",
       " 'warning': 1701,\n",
       " 'skits': 5364,\n",
       " 'position': 2702,\n",
       " 'p': 1715,\n",
       " 'payne': 7917,\n",
       " 'earn': 5919,\n",
       " 'teachers': 5222,\n",
       " 'loser': 3274,\n",
       " 'built': 2152,\n",
       " 'comprised': 8956,\n",
       " 'dumped': 6944,\n",
       " 'proceeds': 4707,\n",
       " 'wang': 5033,\n",
       " 'gypo': 6205,\n",
       " 'shaky': 5058,\n",
       " 'translation': 3759,\n",
       " 'exorcist': 6335,\n",
       " 'jarring': 6237,\n",
       " 'repeat': 3228,\n",
       " 'glory': 3138,\n",
       " 'cries': 6288,\n",
       " 'claim': 2270,\n",
       " 'adults': 1449,\n",
       " 'balanced': 6435,\n",
       " 'print': 2463,\n",
       " 'demon': 2645,\n",
       " 'kansas': 5189,\n",
       " 'suffers': 2453,\n",
       " 'compete': 6091,\n",
       " 'fairbanks': 6535,\n",
       " 'palestinian': 8159,\n",
       " 'worldwide': 9048,\n",
       " 'stiller': 3738,\n",
       " 'varied': 7086,\n",
       " 'christmas': 949,\n",
       " 'headache': 6152,\n",
       " 'beth': 8450,\n",
       " 'jigsaw': 8224,\n",
       " 'switched': 6601,\n",
       " 'ruin': 2428,\n",
       " 'alcoholic': 4507,\n",
       " 'bashing': 6327,\n",
       " 'carey': 5730,\n",
       " 'kingdom': 4434,\n",
       " 'overacting': 4746,\n",
       " 'dubbing': 3047,\n",
       " 'happening': 1429,\n",
       " 'billy': 1454,\n",
       " 'raising': 4487,\n",
       " 'springer': 5329,\n",
       " 'often': 397,\n",
       " 'tension': 1056,\n",
       " 'prostitutes': 8042,\n",
       " 'stiles': 7132,\n",
       " 'up': 57,\n",
       " 'kornbluth': 8360,\n",
       " 'jump': 1752,\n",
       " 'ignores': 7371,\n",
       " 'caution': 8474,\n",
       " 'speaker': 9900,\n",
       " 'rushed': 3303,\n",
       " 'accomplish': 5232,\n",
       " 'vicious': 3771,\n",
       " 'subsequent': 3651,\n",
       " 'remarkable': 1710,\n",
       " 'kriemhild': 9169,\n",
       " 'hank': 4092,\n",
       " 'ability': 1233,\n",
       " 'breathe': 8092,\n",
       " 'has': 48,\n",
       " 'qualify': 7299,\n",
       " 'enhances': 9269,\n",
       " 'basket': 6433,\n",
       " 'despicable': 5631,\n",
       " 'exploited': 6744,\n",
       " 'brooks': 2820,\n",
       " 'heroine': 1802,\n",
       " 'gregory': 5133,\n",
       " 'worrying': 8059,\n",
       " 'astronauts': 8974,\n",
       " 'fell': 1560,\n",
       " 'lasted': 4301,\n",
       " 'sadistic': 3884,\n",
       " 'hardships': 9729,\n",
       " 'favorite': 503,\n",
       " 'bold': 4215,\n",
       " 'principle': 6312,\n",
       " 'reluctant': 5469,\n",
       " 'pokes': 9730,\n",
       " 'ahmad': 8684,\n",
       " 'raised': 2814,\n",
       " 'stretched': 4966,\n",
       " 'effect': 934,\n",
       " 'sutherland': 2956,\n",
       " 'maintain': 4522,\n",
       " 'suggested': 5037,\n",
       " 'lange': 9102,\n",
       " 'hepburn': 5492,\n",
       " 'scene': 135,\n",
       " 'des': 6069,\n",
       " 'entry': 3167,\n",
       " 'insects': 8984,\n",
       " 'alike': 3084,\n",
       " 'book': 268,\n",
       " 'nicholson': 3856,\n",
       " 'honestly': 1227,\n",
       " 'sitcom': 2874,\n",
       " 'decision': 2129,\n",
       " 'this': 12,\n",
       " 'guest': 3372,\n",
       " 'doesnt': 9249,\n",
       " 'iconic': 5515,\n",
       " 'drunk': 1794,\n",
       " 'alexandra': 8195,\n",
       " 'spoon': 9926,\n",
       " 'impressions': 8363,\n",
       " 'hope': 439,\n",
       " 'sure': 250,\n",
       " 'comedic': 1687,\n",
       " 'fleming': 9314,\n",
       " 'crushed': 7510,\n",
       " 'larry': 2646,\n",
       " 'boogeyman': 8046,\n",
       " 'confusion': 2901,\n",
       " 'unconventional': 7691,\n",
       " 'produced': 1038,\n",
       " 'groan': 9731,\n",
       " 'cheat': 8877,\n",
       " 'goldblum': 6004,\n",
       " 'baldwin': 5052,\n",
       " 'ships': 5098,\n",
       " 'aniston': 7852,\n",
       " 'fond': 4205,\n",
       " 'linda': 4093,\n",
       " 'dim': 6023,\n",
       " 'caring': 2868,\n",
       " 'maid': 4899,\n",
       " 'creep': 4589,\n",
       " 'own': 204,\n",
       " 'owe': 8683,\n",
       " 'tremendous': 3496,\n",
       " 'facts': 2263,\n",
       " 'slice': 5077,\n",
       " 'size': 3540,\n",
       " 'june': 4620,\n",
       " 'lecture': 9122,\n",
       " 'askey': 9150,\n",
       " 'pathos': 6820,\n",
       " 'spliced': 9889,\n",
       " 'm': 144,\n",
       " 'murdered': 1990,\n",
       " 'clooney': 6692,\n",
       " 'cruise': 3466,\n",
       " 'entertain': 2799,\n",
       " 'kim': 2379,\n",
       " 'tackle': 8091,\n",
       " 'prove': 1956,\n",
       " 'school': 383,\n",
       " 'musicians': 5226,\n",
       " 'doing': 395,\n",
       " 'composition': 7261,\n",
       " 'odds': 3974,\n",
       " 'blend': 3859,\n",
       " 'dark': 454,\n",
       " 'richards': 4596,\n",
       " 'sit': 849,\n",
       " 'hits': 1912,\n",
       " 'apt': 7133,\n",
       " 'considering': 1053,\n",
       " 'mountain': 2494,\n",
       " 'displays': 3823,\n",
       " 'insist': 6736,\n",
       " 'blondell': 7880,\n",
       " 'epitome': 8004,\n",
       " 'kilmer': 8258,\n",
       " 'marion': 4802,\n",
       " 'zane': 5666,\n",
       " 'after': 102,\n",
       " 'visual': 1088,\n",
       " 'superficial': 3886,\n",
       " 'biblical': 5950,\n",
       " 'broadway': 2113,\n",
       " 'jessica': 2812,\n",
       " 'heap': 7863,\n",
       " 'dvd': 277,\n",
       " 'amateur': 2324,\n",
       " 'ongoing': 8726,\n",
       " 'emphasize': 8406,\n",
       " 'julia': 2591,\n",
       " 'wong': 6328,\n",
       " 'adequately': 9045,\n",
       " 'cameraman': 6912,\n",
       " 'pazu': 9958,\n",
       " 'ebert': 5647,\n",
       " 'guitar': 5395,\n",
       " 'face': 387,\n",
       " 'directing': 918,\n",
       " 'programs': 5811,\n",
       " 'dennis': 2741,\n",
       " 'lamas': 9392,\n",
       " 'spit': 6238,\n",
       " 'daughter': 539,\n",
       " 'elaborate': 3711,\n",
       " 'forward': 910,\n",
       " 'cook': 2966,\n",
       " 'granted': 2449,\n",
       " 'carrey': 3429,\n",
       " 'orphan': 7693,\n",
       " 'colonial': 9694,\n",
       " 'jonathan': 4312,\n",
       " 'brilliance': 3455,\n",
       " 'treats': 4377,\n",
       " 'unhappy': 4422,\n",
       " 'hamlet': 3163,\n",
       " 'topic': 2972,\n",
       " 'elephant': 4302,\n",
       " 'advise': 4609,\n",
       " 'bacall': 4630,\n",
       " 'br': 8,\n",
       " 'hired': 2595,\n",
       " 'bust': 7488,\n",
       " 'stages': 5499,\n",
       " 'gadgets': 9959,\n",
       " 'rooney': 4765,\n",
       " 'depicted': 2363,\n",
       " 'instance': 1812,\n",
       " 'neighbors': 4677,\n",
       " 'coach': 4182,\n",
       " 'colored': 6502,\n",
       " 'deer': 5874,\n",
       " 'shown': 607,\n",
       " 'kidman': 5456,\n",
       " 'drift': 7742,\n",
       " 'watches': 3618,\n",
       " 'yell': 7625,\n",
       " 'gods': 5916,\n",
       " 'fluid': 6810,\n",
       " 'bitter': 2888,\n",
       " 'depict': 6309,\n",
       " 'points': 746,\n",
       " 'saves': 3205,\n",
       " 'reiser': 7127,\n",
       " 'rings': 2634,\n",
       " 'lock': 5215,\n",
       " 'offend': 7328,\n",
       " 'checking': 3275,\n",
       " 'ira': 6805,\n",
       " 'uncle': 1605,\n",
       " 'avenge': 7957,\n",
       " 'lukas': 5688,\n",
       " 'leave': 551,\n",
       " 'cheap': 693,\n",
       " 'scarecrow': 4326,\n",
       " 'executives': 7038,\n",
       " 'suggestive': 9474,\n",
       " 'reasonable': 3749,\n",
       " 'seats': 7085,\n",
       " 'tour': 3042,\n",
       " 'colorful': 3183,\n",
       " 'hitchcock': 2374,\n",
       " 'avoid': 782,\n",
       " 'design': 1572,\n",
       " 'destroy': 2293,\n",
       " 'persona': 3484,\n",
       " 'undead': 5909,\n",
       " 'marshall': 5088,\n",
       " 'alfred': 4828,\n",
       " 'preserved': 9724,\n",
       " 'repeatedly': 3699,\n",
       " 'tasty': 8341,\n",
       " 'there': 41,\n",
       " 'seuss': 7864,\n",
       " 'playwright': 7144,\n",
       " 'tastes': 5150,\n",
       " 'izzard': 8710,\n",
       " 'hilarity': 5752,\n",
       " 'rapes': 8858,\n",
       " 'named': 759,\n",
       " 'prepare': 5282,\n",
       " 'vulnerability': 8918,\n",
       " 'marriage': 1322,\n",
       " 'naughty': 5384,\n",
       " 'cassie': 7973,\n",
       " 'reynolds': 3444,\n",
       " 'flicks': 1522,\n",
       " 'tempted': 5719,\n",
       " 'golf': 8727,\n",
       " 'owns': 6336,\n",
       " 'mouthed': 6675,\n",
       " 'delia': 8827,\n",
       " 'stevenson': 8512,\n",
       " 'walters': 9933,\n",
       " 'exotic': 4130,\n",
       " 'esquire': 6403,\n",
       " 'radio': 1835,\n",
       " 'expert': 2750,\n",
       " 'craft': 3852,\n",
       " 'decency': 8464,\n",
       " 'peak': 5078,\n",
       " 'twentieth': 9978,\n",
       " 'slipped': 9421,\n",
       " 'guevara': 7485,\n",
       " 'caricatures': 5205,\n",
       " 'march': 4211,\n",
       " 'davies': 3456,\n",
       " 'excellent': 320,\n",
       " 'with': 18,\n",
       " 'torturing': 8511,\n",
       " 'friendly': 2519,\n",
       " 'usually': 622,\n",
       " 'influenced': 4047,\n",
       " 'moment': 547,\n",
       " 'biker': 6896,\n",
       " 'diving': 8031,\n",
       " 'isabelle': 7241,\n",
       " 'thin': 1507,\n",
       " 'cagney': 3561,\n",
       " 'tiny': 2328,\n",
       " 'pub': 8551,\n",
       " 'tim': 1709,\n",
       " 'stuff': 530,\n",
       " 'harry': 1216,\n",
       " 'assigned': 4969,\n",
       " 'tapes': 5962,\n",
       " 'them': 97,\n",
       " 'leslie': 2790,\n",
       " 'shah': 6952,\n",
       " 'wished': 4424,\n",
       " 'conflict': 1906,\n",
       " 'cons': 6019,\n",
       " 'breathless': 9083,\n",
       " 'fast': 690,\n",
       " 'tomei': 6924,\n",
       " 'smooth': 3476,\n",
       " 'beat': 1529,\n",
       " 'brain': 1189,\n",
       " 'filmmaker': 1608,\n",
       " 'each': 254,\n",
       " 'broadcast': 3827,\n",
       " 'gerard': 4565,\n",
       " 'higher': 1813,\n",
       " 'portman': 8812,\n",
       " 'century': 1083,\n",
       " 'apply': 6609,\n",
       " 'camera': 363,\n",
       " 'grab': 3969,\n",
       " 'christie': 5785,\n",
       " 'mccoy': 5352,\n",
       " 'parallel': 4659,\n",
       " 'horny': 6571,\n",
       " 'holidays': 9617,\n",
       " 'despise': 7617,\n",
       " 'emerged': 9621,\n",
       " 'encountered': 6836,\n",
       " 'marie': 2035,\n",
       " 'regard': 2875,\n",
       " 'eyed': 3385,\n",
       " 'alarm': 7914,\n",
       " 'rabbit': 4724,\n",
       " 'performances': 352,\n",
       " 'cutter': 6607,\n",
       " 'toe': 8821,\n",
       " 'admire': 3556,\n",
       " 'stumbling': 9005,\n",
       " 'hoffman': 2589,\n",
       " 'kudos': 3527,\n",
       " 'polished': 4854,\n",
       " 'neutral': 9820,\n",
       " 'carnival': 8706,\n",
       " 'radiation': 7896,\n",
       " 'strung': 7555,\n",
       " 'dubbed': 2255,\n",
       " 'realistic': 804,\n",
       " 'backgrounds': 4241,\n",
       " 'wendigo': 5767,\n",
       " 'detached': 7821,\n",
       " 'wasn': 286,\n",
       " 'severe': 4339,\n",
       " 'tiresome': 4523,\n",
       " 'disaster': 1666,\n",
       " 'ensuing': 9803,\n",
       " 'alter': 6619,\n",
       " 'picks': 2832,\n",
       " 'kurt': 3135,\n",
       " 'ad': 3149,\n",
       " 'proper': 2214,\n",
       " 'reality': 614,\n",
       " 'tomatoes': 4734,\n",
       " 'compelled': 4762,\n",
       " 'sexuality': 3103,\n",
       " 'introduction': 2838,\n",
       " 'plausible': 4756,\n",
       " 'werewolves': 6386,\n",
       " 'services': 7248,\n",
       " 'hence': 3014,\n",
       " 'trivial': 7548,\n",
       " 'above': 736,\n",
       " 'platform': 7296,\n",
       " 'metaphor': 5032,\n",
       " 'psychic': 4791,\n",
       " 'poignant': 3011,\n",
       " 'diverse': 6748,\n",
       " 'terrorism': 8192,\n",
       " 'brains': 3979,\n",
       " 'roughly': 7014,\n",
       " 'wash': 6869,\n",
       " 'interesting': 219,\n",
       " 'burnt': 7230,\n",
       " 'arrival': 4745,\n",
       " 'moe': 5877,\n",
       " 'later': 300,\n",
       " 'anchorman': 9353,\n",
       " 'provided': 2387,\n",
       " 'jeff': 1757,\n",
       " 'waters': 3998,\n",
       " 'witnessing': 7451,\n",
       " 'feared': 9054,\n",
       " 'lend': 6202,\n",
       " 'drake': 4660,\n",
       " 'spaceship': 7911,\n",
       " 'mercy': 5860,\n",
       " 'beef': 9957,\n",
       " 'exciting': 1105,\n",
       " 'become': 410,\n",
       " 'archie': 9770,\n",
       " 'abundance': 7806,\n",
       " 'celebrating': 9653,\n",
       " 'ray': 1444,\n",
       " 'secure': 8094,\n",
       " 'cousins': 8946,\n",
       " 'joseph': 2195,\n",
       " 'adrian': 6634,\n",
       " 'cia': 3587,\n",
       " 'lifeless': 5867,\n",
       " 'waterfall': 9983,\n",
       " 'surroundings': 7122,\n",
       " 'shakespearean': 8843,\n",
       " 'engage': 4506,\n",
       " 'delicate': 5724,\n",
       " 'prisoner': 4694,\n",
       " 'gritty': 2498,\n",
       " 'miserably': 3564,\n",
       " 'bruce': 1406,\n",
       " 'cruelty': 5512,\n",
       " 'spare': 4036,\n",
       " 'stares': 8325,\n",
       " 'swing': 5336,\n",
       " 'phantasm': 7422,\n",
       " 'innuendo': 8810,\n",
       " 'gary': 1920,\n",
       " 'distress': 6660,\n",
       " 'vacation': 2968,\n",
       " 'possibility': 4031,\n",
       " 'hats': 6033,\n",
       " 'guests': 5470,\n",
       " 'listing': 9324,\n",
       " 'lundgren': 4545,\n",
       " 'selling': 3457,\n",
       " 'outer': 4140,\n",
       " 'voyager': 7270,\n",
       " 'corn': 6195,\n",
       " 'admits': 7473,\n",
       " 'respond': 8061,\n",
       " 'charismatic': 3343,\n",
       " 'sundance': 6828,\n",
       " 'kinky': 9216,\n",
       " 'melissa': 5695,\n",
       " 'hating': 7303,\n",
       " 'contest': 4922,\n",
       " 'overt': 9483,\n",
       " 'message': 730,\n",
       " 'shrill': 8774,\n",
       " 'having': 256,\n",
       " 'boat': 2037,\n",
       " 'narrative': 1300,\n",
       " 'intention': 3397,\n",
       " 'stranded': 6345,\n",
       " 'galactica': 7060,\n",
       " 'imagined': 3783,\n",
       " 'basis': 2819,\n",
       " 'work': 159,\n",
       " 'home': 340,\n",
       " 'spoil': 2333,\n",
       " 'manners': 7006,\n",
       " 'fanatic': 6933,\n",
       " 'mixes': 8433,\n",
       " 'joe': 866,\n",
       " 'compared': 1061,\n",
       " 'abu': 8196,\n",
       " 'chore': 9583,\n",
       " 'franco': 4351,\n",
       " 'collins': 7145,\n",
       " 'daylight': 7071,\n",
       " 'down': 181,\n",
       " 'comical': 2808,\n",
       " 'upside': 6792,\n",
       " 'fat': 1886,\n",
       " 'encounters': 3262,\n",
       " 'puerto': 6278,\n",
       " 'raving': 9644,\n",
       " 'challenge': 2883,\n",
       " 'meet': 885,\n",
       " 'hardly': 968,\n",
       " 'sentinel': 5181,\n",
       " 'connecting': 9410,\n",
       " 'cloak': 7822,\n",
       " 'indie': 2671,\n",
       " 'taxi': 4582,\n",
       " 'adds': 1591,\n",
       " 'expectations': 1367,\n",
       " 'save': 595,\n",
       " 'senator': 7813,\n",
       " 'kidnaps': 8365,\n",
       " 'brett': 7187,\n",
       " 'questions': 1181,\n",
       " 'dustin': 6366,\n",
       " 'rifle': 5199,\n",
       " 'heard': 549,\n",
       " 'berenger': 8316,\n",
       " 'perspectives': 8284,\n",
       " 'chief': 2208,\n",
       " 'lamarr': 9304,\n",
       " 'weekend': 2392,\n",
       " 'norwegian': 9415,\n",
       " 'unimaginative': 6204,\n",
       " 'empathize': 9390,\n",
       " 'enough': 194,\n",
       " 'towns': 8211,\n",
       " 'lately': 4410,\n",
       " 'chosen': 2196,\n",
       " 'jabba': 7755,\n",
       " 'quickly': 924,\n",
       " 'lazy': 2841,\n",
       " 'l': 1496,\n",
       " 'lugosi': 2509,\n",
       " 'induced': 9103,\n",
       " 'remembering': 6593,\n",
       " 'comedies': 1265,\n",
       " 'sour': 7399,\n",
       " 'problem': 436,\n",
       " 'humor': 477,\n",
       " 'furthermore': 4070,\n",
       " 'maria': 2760,\n",
       " 'spend': 1123,\n",
       " 'cruella': 8312,\n",
       " 'thirty': 3240,\n",
       " 'villagers': 9280,\n",
       " 'continuously': 8607,\n",
       " 'neeson': 8904,\n",
       " 'santa': 1876,\n",
       " 'investigator': 7098,\n",
       " 'proved': 2065,\n",
       " 'heroin': 8691,\n",
       " 'moving': 714,\n",
       " 'substance': 2295,\n",
       " 'amid': 8338,\n",
       " 'denying': 8567,\n",
       " 'fade': 5426,\n",
       " 'love': 116,\n",
       " 'favourite': 1623,\n",
       " 'canceled': 6650,\n",
       " 'progressed': 7475,\n",
       " 'burke': 6235,\n",
       " 'deceased': 4987,\n",
       " 'vcr': 7929,\n",
       " 'clueless': 5660,\n",
       " 'poison': 5178,\n",
       " 'skipped': 9733,\n",
       " 'horrified': 6705,\n",
       " 'dozens': 4176,\n",
       " 'farmer': 5497,\n",
       " 'keeper': 8671,\n",
       " 'se': 6186,\n",
       " 'landed': 5853,\n",
       " 'cradle': 8627,\n",
       " 'starting': 1843,\n",
       " 'bedroom': 4027,\n",
       " 'native': 2144,\n",
       " 'stunning': 1353,\n",
       " 'forgiveness': 7578,\n",
       " 'conscious': 5102,\n",
       " 'sober': 8022,\n",
       " 'unfolding': 7434,\n",
       " 'expand': 9020,\n",
       " 'furious': 4923,\n",
       " 'way': 95,\n",
       " 'castle': 1585,\n",
       " 'canon': 9903,\n",
       " 'dental': 8903,\n",
       " 'pay': 973,\n",
       " 'magazines': 8093,\n",
       " 'feeling': 538,\n",
       " 'overshadowed': 8259,\n",
       " 'righteous': 8764,\n",
       " 'limits': 4298,\n",
       " 'lesser': 2880,\n",
       " 'safely': 7344,\n",
       " 'phil': 5089,\n",
       " 'randy': 4763,\n",
       " 'peck': 5415,\n",
       " 'fire': 942,\n",
       " 'building': 1396,\n",
       " 'chen': 8396,\n",
       " 'exactly': 606,\n",
       " 'partly': 3490,\n",
       " 'female': 653,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2idx = ## create the word-to-index dictionary here\n",
    "word2idx = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2idx[word] = i\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to vector function\n",
    "\n",
    "Now we can write a function that converts a some text to a word vector. The function will take a string of words as input and return a vector with the words counted up. Here's the general algorithm to do this:\n",
    "\n",
    "* Initialize the word vector with [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html), it should be the length of the vocabulary.\n",
    "* Split the input string of text into a list of words with `.split(' ')`. Again, if you call `.split()` instead, you'll get slightly different results than what we show here.\n",
    "* For each word in that list, increment the element in the index associated with that word, which you get from `word2idx`.\n",
    "\n",
    "**Note:** Since all words aren't in the `vocab` dictionary, you'll get a key error if you run into one of those words. You can use the `.get` method of the `word2idx` dictionary to specify a default returned value when you make a key error. For example, `word2idx.get(word, None)` returns `None` if `word` doesn't exist in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_vector(text):\n",
    "    word_vector = np.zeros(len(vocab), dtype=np.int_)\n",
    "    #word_vector *0 = 0\n",
    "    for word in text.split(\" \"):\n",
    "        if word in vocab:\n",
    "            word_vector[word2idx[word]] += 1\n",
    "        #else:\n",
    "    return np.array(word_vector)        \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def text_to_vector(text):\n",
    "    word_vector = np.zeros(len(vocab),dtype=np.int_)\n",
    "    for word in text.split(\" \"):\n",
    "        idx = word2idx.get(word,None)\n",
    "        if idx == None:\n",
    "            continue\n",
    "        else:\n",
    "            word_vector[word2idx[word]] += 1\n",
    "    return np.array(word_vector)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do this right, the following code should return\n",
    "\n",
    "```\n",
    "text_to_vector('The tea is for a party to celebrate '\n",
    "               'the movie so she has no time for a cake')[:65]\n",
    "                   \n",
    "array([0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0,\n",
    "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
    "```       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_vector('The tea is for a party to celebrate '\n",
    "               'the movie so she has no time for a cake')[:65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run through our entire review data set and convert each review to a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.zeros((len(reviews), len(vocab)), dtype=np.int_)\n",
    "for ii, (_, text) in enumerate(reviews.iterrows()):\n",
    "    word_vectors[ii] = text_to_vector(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18,   9,  27,   1,   4,   4,   6,   4,   0,   2,   2,   5,   0,\n",
       "          4,   1,   0,   2,   0,   0,   0,   0,   0,   0],\n",
       "       [  5,   4,   8,   1,   7,   3,   1,   2,   0,   4,   0,   0,   0,\n",
       "          1,   2,   0,   0,   1,   3,   0,   0,   0,   1],\n",
       "       [ 78,  24,  12,   4,  17,   5,  20,   2,   8,   8,   2,   1,   1,\n",
       "          2,   8,   0,   5,   5,   4,   0,   2,   1,   4],\n",
       "       [167,  53,  23,   0,  22,  23,  13,  14,   8,  10,   8,  12,   9,\n",
       "          4,  11,   2,  11,   5,  11,   0,   5,   3,   0],\n",
       "       [ 19,  10,  11,   4,   6,   2,   2,   5,   0,   1,   2,   3,   1,\n",
       "          0,   0,   0,   3,   1,   0,   1,   0,   0,   0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out the first 5 word vectors\n",
    "word_vectors[:5, :23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation, Test sets\n",
    "\n",
    "Now that we have the word_vectors, we're ready to split our data into train, validation, and test sets. Remember that we train on the train data, use the validation data to set the hyperparameters, and at the very end measure the network performance on the test data. Here we're using the function `to_categorical` from TFLearn to reshape the target data so that we'll have two output units and can classify with a softmax activation function. We actually won't be creating the validation set here, TFLearn will do that for us later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = (labels=='positive').astype(np.int_)\n",
    "records = len(labels)\n",
    "\n",
    "shuffle = np.arange(records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "trainX, trainY = word_vectors[train_split,:], to_categorical(Y.values[train_split], 2)\n",
    "testX, testY = word_vectors[test_split,:], to_categorical(Y.values[test_split], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.],\n",
       "       [ 1.,  1.],\n",
       "       [ 1.,  1.],\n",
       "       ..., \n",
       "       [ 1.,  1.],\n",
       "       [ 1.,  1.],\n",
       "       [ 1.,  1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "[TFLearn](http://tflearn.org/) lets you build the network by [defining the layers](http://tflearn.org/layers/core/). \n",
    "\n",
    "### Input layer\n",
    "\n",
    "For the input layer, you just need to tell it how many units you have. For example, \n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, 100])\n",
    "```\n",
    "\n",
    "would create a network with 100 input units. The first element in the list, `None` in this case, sets the batch size. Setting it to `None` here leaves it at the default batch size.\n",
    "\n",
    "The number of inputs to your network needs to match the size of your data. For this example, we're using 10000 element long vectors to encode our input data, so we need 10000 input units.\n",
    "\n",
    "\n",
    "### Adding layers\n",
    "\n",
    "To add new hidden layers, you use \n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, n_units, activation='ReLU')\n",
    "```\n",
    "\n",
    "This adds a fully connected layer where every unit in the previous layer is connected to every unit in this layer. The first argument `net` is the network you created in the `tflearn.input_data` call. It's telling the network to use the output of the previous layer as the input to this layer. You can set the number of units in the layer with `n_units`, and set the activation function with the `activation` keyword. You can keep adding layers to your network by repeated calling `net = tflearn.fully_connected(net, n_units)`.\n",
    "\n",
    "### Output layer\n",
    "\n",
    "The last layer you add is used as the output layer. Therefore, you need to set the number of units to match the target data. In this case we are predicting two classes, positive or negative sentiment. You also need to set the activation function so it's appropriate for your model. Again, we're trying to predict if some input data belongs to one of two classes, so we should use softmax.\n",
    "\n",
    "```\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "```\n",
    "\n",
    "### Training\n",
    "To set how you train the network, use \n",
    "\n",
    "```\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "Again, this is passing in the network you've been building. The keywords: \n",
    "\n",
    "* `optimizer` sets the training method, here stochastic gradient descent\n",
    "* `learning_rate` is the learning rate\n",
    "* `loss` determines how the network error is calculated. In this example, with the categorical cross-entropy.\n",
    "\n",
    "Finally you put all this together to create the model with `tflearn.DNN(net)`. So it ends up looking something like \n",
    "\n",
    "```\n",
    "net = tflearn.input_data([None, 10])                          # Input\n",
    "net = tflearn.fully_connected(net, 5, activation='ReLU')      # Hidden\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')   # Output\n",
    "net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "model = tflearn.DNN(net)\n",
    "```\n",
    "\n",
    "> **Exercise:** Below in the `build_model()` function, you'll put together the network using TFLearn. You get to choose how many layers to use, how many hidden units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network building\n",
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #### Your code ####\n",
    "    \n",
    "    net = tflearn.input_data([None,10000])\n",
    "    net = tflearn.fully_connected(net, 200, activation='ReLU') \n",
    "    net = tflearn.fully_connected(net, 25, activation='ReLU') \n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    \n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.1, loss='categorical_crossentropy')\n",
    "    #### Your code ####\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing the model\n",
    "\n",
    "Next we need to call the `build_model()` function to actually build the model. In my solution I haven't included any arguments to the function, but you can add arguments so you can change parameters in the model if you want.\n",
    "\n",
    "> **Note:** You might get a bunch of warnings here. TFLearn uses a lot of deprecated code in TensorFlow. Hopefully it gets updated to the new TensorFlow version soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Now that we've constructed the network, saved as the variable `model`, we can fit it to the data. Here we use the `model.fit` method. You pass in the training features `trainX` and the training targets `trainY`. Below I set `validation_set=0.1` which reserves 10% of the data set as the validation set. You can also set the batch size and number of epochs with the `batch_size` and `n_epoch` keywords, respectively. Below is the code to fit our the network to our word vectors.\n",
    "\n",
    "You can rerun `model.fit` to train the network further if you think you can increase the validation accuracy. Remember, all hyperparameter adjustments must be done using the validation set. **Only use the test set after you're completely done training the network.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m1.38630\u001b[0m\u001b[0m | time: 1.324s\n",
      "\u001b[2K\r",
      "| SGD | epoch: 003 | loss: 1.38630 - acc: 0.4631 -- iter: 02816/18000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-377f2f09ec6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/tflearn/lib/python3.5/site-packages/tflearn/models/dnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    214\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                          callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tflearn/lib/python3.5/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[1;32m    337\u001b[0m                                                        \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                                                        show_metric)\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                             \u001b[0;31m# Update training state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tflearn/lib/python3.5/site-packages/tflearn/helpers/trainer.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[0;32m--> 818\u001b[0;31m                                              feed_batch)\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# Retrieve loss value from summary string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tflearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tflearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tflearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tflearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/tflearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model.fit(trainX, trainY, validation_set=0.2, show_metric=True, batch_size=128, n_epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "After you're satisified with your hyperparameters, you can run the network on the test set to measure its performance. Remember, *only do this after finalizing the hyperparameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == testY[:,0], axis=0)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out your own text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function that uses your model to predict sentiment\n",
    "def test_sentence(sentence):\n",
    "    positive_prob = model.predict([text_to_vector(sentence.lower())])[0][1]\n",
    "    print('Sentence: {}'.format(sentence))\n",
    "    print('P(positive) = {:.3f} :'.format(positive_prob), \n",
    "          'Positive' if positive_prob > 0.5 else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Moonlight is by far the best movie of 2016.\n",
      "P(positive) = 0.500 : Positive\n",
      "Sentence: It's amazing anyone could be talented enough to make something this spectacularly awful\n",
      "P(positive) = 0.500 : Positive\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Moonlight is by far the best movie of 2016.\"\n",
    "test_sentence(sentence)\n",
    "\n",
    "sentence = \"It's amazing anyone could be talented enough to make something this spectacularly awful\"\n",
    "test_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
